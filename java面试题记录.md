### java面试题记录

1. #### 说一下JVM的主要组成部分及其作用？

JVM包含两个子系统和两个组件，两个子系统为Classs loader(类装载)、Execution engine(执行引擎)；两个组件为 Runtime data area(运行时数据区)、Native Interface(本地接口).
  Class loader(类装载):根据给定的全限定名类名来装载class文件到Runtime data area中的method area。
  Execution engine(执行引擎)：执行classes中的指令。
  Native Interface(本地接口)：与 native libraries交互，是其它编程语言交互的接口。
  Runtime data area(运行时数据区域)：这就是我们常说的 JVM 的内存。
作用：首先通过编译器吧Java代码转换为字节码，类加载器（ClassLoader）再把字节码加载到内存中，将其放在运行时数据区（Runtime data area）的方法区内，而字节码文件只是JVM的一套指令集规范，并不能直接交给底层操作系统执行，因此需要特定的命令解析器执行引擎（Execution engine），将字节码翻译成底层系统指令，再交由CPU去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。

2. #### mysql索引设计的原则有哪些？

   1.适合索引的列是出现在where子句中的列，或者连接子句中指定的列；

   2.基数较小的类，索引效果较差，没有必要在此列建立索引；

   3.使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间；

   4.不要过度索引，索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。索引只保持需要的索引有利于查询即可。

3. #### 并发编程三要素是什么？在Java程序中怎么保证多线程的运行安全？

   并发编程的三要素（线程安全性问题体现在）：

   原子性：原子，即一个不可再分割的颗粒。原子性指的是一个或多个操作要么全部执行成功要么全部执行失败。

   可见性：一个线程对共享变量的修改，另一个线程能够立刻看到。（synchronized, volatile）

   有序性：程序执行的顺序按照代码的先后顺序。（处理器可能会对指令进行重排序）

   出现线程安全问题的原因：

   - 线程切换带来的原子性问题
   - 缓存导致的可见性问题
   - 编程优化带来的有序性问题

   解决办法：

   JDK Atomic开头的原子类、sychroneizd、LOCK，可以解决原子性问题

   synchronized、volatile、LOCK，可以解决可见性问题

   Happens-Before 规则可以解决有序性问题

4. #### 创建索引的原则

1）最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询（>、<、between、like）就停止匹配，比如 a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

2）较频繁作为查询条件的字段才去创建索引。

3）更新频繁字段不适合创建索引。

4）不能有效区分数据的列不适合做为索引列（如性别，男、女、未知，区分度太低）。

5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a, b)的索引，那么只需要修改原来的索引即可。

6）定义有外键的数据列一定要建立索引。

7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。

8）对于定义为text、image和bit的数据类型的列不要建立索引。

5. #### 请解释 Spring Bean 的生命周期？

Spring Bean 的生命周期简单易懂。在一个 bean 实例被初始化时，需要执行一系列的初始化操作以达到可用的状态。同样的，当一个 bean 不在被调用时需要进行相关的析构操作，并从 bean 容器中移除。
Spring bean factory 负责管理在 spring 容器中被创建的 bean 的生命周期。Bean 的生命周期由两组回调（call back）方法组成。

1. 初始化之后调用的回调方法。
2. 销毁之前调用的回调方法

Spring 框架提供了以下四种方式来管理 bean 的生命周期事件：

InitializingBean 和 DisposableBean 回调接口；
针对特殊行为的其他 Aware 接口；
Bean 配置文件中的 Custom init()方法和 destroy()方法；
@PostConstruct 和@PreDestroy 注解方式。

参考：

http://t.csdn.cn/HeaVd

6. #### zookeeper 是如何保证事务的顺序一致性的？

   zookeeper 采用了递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch（时期; 纪元; 世; 新时代）用来标识 leader 是否发生改变，如果有新的 leader 产生出来，epoch 会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。

7. #### MQ中，有哪些情况消息可能会成为死信？RabbitMQ中的延迟队列是怎么实现的？

   消息成为死信的三种情况：
   1. 队列消息长度到达限制；
   2. 消费者拒接消费消息，basicNack/basicReject,并且不把消息重新放入原目标队列,requeue=false；
   3. 原队列存在消息过期设置，消息到达超时时间未被消费；
       RabbitMQ通过TTL+死信队列 组合实现延迟队列。

8. #### MQ中如何防止消息重复消费、以及如何处理消息积压？

   首先需要知道幂等性的概念了，幂等性指一次和多次请求某一个资源，对于资源本身应该具有同样的结果。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。在MQ中指，消费多条相同的消息，得到与消费该消息一次相同的结果。我们可以使用乐观锁机制达到消息幂等性保障，减少消息重复消费的情况。
   然后就是要知道消息积压产生的原因有哪些：

   1. 消费者宕机积压；
   2. 消费者消费能力不足积压；
   3. 发送者发流量太大；
   解决消息积压的方案：

   1. 上线更多的消费者,进行正常消费；
   2. 上线专门的队列消费服务；
   3. 将消息先批量取出来,记录数据库,再慢慢处理；

9. #### Redis如何做内存优化？

可以好好利用Hash,list, sorted set,set 等集合类型数据结构，因为通常情况下很多的 Key-Value可以用紧凑的方式存放在一起，尽可能使用散列表(hash),散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的讲你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面。

10. #### 如何使用 Spring Boot 实现异常处理？

Spring 提供了一种使用 ControllerAdvice 处理异常的非常有用的方法。 我们通过实现一个ControlerAdvice 类，来处理控制器类抛出的所有异常。

#### 11. 为什么我们调用 start() 方法时会执行 run() 方法，为什么我们 不能直接调用 run() 方法？

new 一个 Thread，线程进入了新建状态。调用 start() 方法，会启动一个线程 并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工 作。

而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执 行，并不会在某个线程中执行它，所以这并不是多线程工作。 

总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。 

12. #### zookeeper脑裂是什么原因导致的？zookeeper是如何解决"脑裂"问题的？

简单点来说，脑裂 (Split-Brain) 就是比如当你的 cluster 里面有两个节点，它们都知道在这个 cluster 里需要选举出一个 master。那么当它们两个之间的通信完全没有问题的时候，就会达成共识，选出其中一个作为 master。但是如果它们之间的通信出了问题，那么两个结点都会觉得现在没有 master，所以每个都把自己选举成 master，于是 cluster 里面就会有两个 master。

对于 Zookeeper 来说有一个很重要的问题，就是到底是根据一个什么样的情况来判断一个节点死亡 down 掉了。 在分布式系统中这些都是有监控者来判断的，但是监控者也很难判定其他的节点的状态，唯一一个可靠的途径就是心跳，Zookeeper 也是使用心跳来判断客户端是否仍然活着。

使用 ZooKeeper 来做 Leader HA 基本都是同样的方式：每个节点都尝试注册一个象征 leader 的临时节点，其他没有注册成功的则成为 follower，并且通过 watch 机制 (这里有介绍) 监控着leader所创建的临时节点，Zookeeper 通过内部心跳机制来确定 leader 的状态，一旦 leader 出现意外 Zookeeper 能很快获悉并且通知其他的 follower，其他 flower 在之后作出相关反应，这样就完成了一个切换，这种模式也是比较通用的模式，基本大部分都是这样实现的。但是这里面有个很严重的问题，如果注意不到会导致短暂的时间内系统出现脑裂，因为心跳出现超时可能是 leader 挂了，但是也可能是 zookeeper 节点之间网络出现了问题，导致 leader 假死的情况，leader其实并未死掉，但是与 ZooKeeper 之间的网络出现问题导致Zookeeper 认为其挂掉了然后通知其他节点进行切换，这样 follower 中就有一个成为了 leader，但是原本的 leader 并未死掉，这时候 client 也获得 leader 切换的消息，但是仍然会有一些延时，zookeeper 需要通讯需要一个一个通知，这时候整个系统就很混乱可能有一部分 client 已经通知到了连接到新的 leader 上去了，有的 client 仍然连接在老的 leader 上，如果同时有两个 client 需要对 leader 的同一个数据更新，并且刚好这两个 client 此刻分别连接在新老的 leader 上，就会出现很严重问题。

在多机房多节点构成的zookeeper集群可能会因为网络通信问题，因为选举机制产生多个leader节点的情况，这就是脑裂。

zooKeeper **默认采用了 Quorums 这种方式来防止"脑裂"现象**，即只有集群中超过半数节点投票才能选举出 Leader。 zookeeper采用了paxos算法，在进行leader选举时，假设某个 leader 假死，其余的 followers 选举出了一个新的 leader。这时，旧的leader 复活并且仍然认为自己是 leader，这个时候它向其他 followers 发出写请求也是会被拒绝的。因为每当新 leader 产生时，会生成一个 epoch 标号(标识当前属于那个 leader 的统治时期)，这个epoch是递增的，followers 如果确认了新的leader 存在，知道其 epoch，就会拒绝 epoch 小于现任 leader epoch 的所有请求。那有没有 follower 不知道新的 leader 存在呢，有可能，但肯定不是大多数，否则新 leader 无法产生。Zookeeper 的写也遵循 quorum 机制，因此，得不到大多数支持的写是无效的，旧 leader 即使各种认为自己是 leader，依然没有什么作用。 这样的方式可以确保 leader 的唯一性,要么选出唯一的一个 leader,要么选举失败。在 zookeeper 中 Quorums 有2个作用：

- 集群中最少的节点数用来选举 leader 保证集群可用。
- 通知客户端数据已经安全保存前集群中最少数量的节点数已经保存了该数据。一旦这些节点保存了该数据，客户端将被通知已经安全保存了，可以继续其他任务。而集群中剩余的节点将会最终也保存了该数据。

参考链接：https://blog.csdn.net/qq_31960623/article/details/119713311

13. #### ThreadLocal 是什么？有哪些使用场景？

ThreadLocal 是一个本地线程副本变量工具类，在每个线程中都创建了一个 ThreadLocalMap 对象，简单说 ThreadLocal 就是一种以空间换时间的做法， 每个线程可以访问自己内部 ThreadLocalMap 对象内的 value。通过这种方 式，避免资源在多线程间共享。 

原理：线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java提供ThreadLocal类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。

经典的使用场景是为每个线程分配一个 JDBC 连接 Connection。这样就可以保 证每个线程的都在各自的 Connection 上进行数据库的操作，不会出现 A 线程关了 B线程正在使用的 Connection； 还有 Session 管理 等问题。 



#### 14. kafka高性能的原因

磁盘顺序读写：kafka消息不能修改以及不会从文件中间删除保证了磁盘顺序读，kafka的消息写入文件都是追加在文件末尾，不会写入文件中的某个位置(随机写)保证了磁盘顺序写。

数据传输的零拷贝。
读写数据的批量batch处理以及压缩传输。



后面可能会加入的面试点：

也是我以前面试过没有答上来的：

1. HashMap 与 ConCurrentHashMap
2. Feign与OpenFeign
3. java的引用类型有哪些：强软虚弱

4. **为什么要对Topic下数据进行分区存储？** 

1、commit log文件会受到所在机器的文件系统大小的限制，分区之后可以将不同的分区放在不同的机器上，相当于对 

数据做了分布式存储，理论上一个topic可以处理任意数量的数据。 

2、为了**提高并行度**。 



